{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Load Environment Variables\n",
    "load_dotenv(\".env\")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "\n",
    "#setting up logging\n",
    "if not logging.getLogger().hasHandlers():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI()\n",
    "model = 'gpt-4o-mini'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create an assistant** \n",
    "\n",
    "Only needed once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing Assistant and Thread IDs\n"
     ]
    }
   ],
   "source": [
    "# Check if assistant_id and thread_id exist\n",
    "if os.path.exists('assistant_thread_ids.txt'):\n",
    "    # Read assistant_id and thread_id from the file\n",
    "    with open('assistant_thread_ids.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        assistant_id = lines[0].strip()\n",
    "        thread_id = lines[1].strip()\n",
    "    print(\"Using existing Assistant and Thread IDs\")\n",
    "else:\n",
    "    # Create an Assistant (Run Once)\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name='Code Completion Assistant',\n",
    "        instructions='As an expert code assistant, complete the provided code cell based on the instructions and previous code. Return only the code without any markdown, code fences, or explanations. The completed code should be enclosed in code fences (```python ... ```).',\n",
    "        tools=[{\"type\": \"code_interpreter\"}],\n",
    "        model=model,\n",
    "    )\n",
    "    assistant_id = assistant.id\n",
    "    print(\"Assistant ID:\", assistant_id)\n",
    "\n",
    "    # Create a new Thread\n",
    "    thread = client.beta.threads.create()\n",
    "    thread_id = thread.id\n",
    "    print(\"Thread ID:\", thread_id)\n",
    "\n",
    "    # Save assistant_id and thread_id to a file\n",
    "    with open('assistant_thread_ids.txt', 'w') as f:\n",
    "        f.write(f\"{assistant_id}\\n\")\n",
    "        f.write(f\"{thread_id}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread ID: thread_mM15QTqqjsYi9ROnyhPjusli\n",
      "Assistant ID: asst_VipgeC5YkRAKLKHl4Jbg3w86\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Thread ID:\", thread_id)       # Print the thread ID\n",
    "print(\"Assistant ID:\", assistant_id) # Print the assistant ID\n",
    "\n",
    "\n",
    "# === Define the Thinking Function ===\n",
    "def thinking(client, thread_id, run_id, sleep_interval=5):\n",
    "    \"\"\"\n",
    "    Waits for a run to complete and prints the elapsed time.\n",
    "    :param client: The OpenAI client object.\n",
    "    :param thread_id: The ID of the thread.\n",
    "    :param run_id: The ID of the run.\n",
    "    :param sleep_interval: Time in seconds to wait between checks.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)  # Retrieve the current status of the run\n",
    "            if run.completed_at:  # Check if the run has completed\n",
    "                elapsed_time = run.completed_at - run.created_at  # Calculate elapsed time\n",
    "                formatted_elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "                print(f\"Run completed in {formatted_elapsed_time}\")\n",
    "                logging.info(f\"Run completed in {formatted_elapsed_time}\")  # Log the elapsed time\n",
    "\n",
    "                # Get messages once Run is completed\n",
    "                messages = client.beta.threads.messages.list(thread_id=thread_id)  # Retrieve all messages in the thread\n",
    "                if messages.data:\n",
    "                    last_message = messages.data[0]  # Assuming the latest message is first\n",
    "                    # Depending on the Assistant API's response structure, adjust the access method\n",
    "                    response = last_message.content.text.value if hasattr(last_message.content, 'text') else last_message.content\n",
    "                    print(f\"Assistant Response: {response}\")\n",
    "                else:\n",
    "                    print(\"No messages found in the thread.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while retrieving the run: {e}\")\n",
    "            break\n",
    "        logging.info(\"Waiting for run to complete...\")\n",
    "        time.sleep(sleep_interval)\n",
    "\n",
    "# === Function to Read Jupyter Notebook ===\n",
    "def read_notebook(file_path):\n",
    "    \"\"\"Read a Jupyter notebook file and parse it into a JSON object.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "    return notebook\n",
    "\n",
    "\n",
    "def extract_cells(notebook):\n",
    "    \"\"\"Extract markdown and code cells, removing large base64 image strings from markdown.\"\"\"\n",
    "    cells = notebook['cells']\n",
    "    paired_cells = []  # List of tuples in the form (markdown_instructions, code_cell)\n",
    "\n",
    "    current_markdown = []\n",
    "    image_pattern = re.compile(r'!\\[.*?\\]\\(data:image/(?:png|jpg|jpeg|gif);base64,[^\\)]+\\)')\n",
    "\n",
    "    for cell in cells:\n",
    "        if cell['cell_type'] == 'markdown':\n",
    "            clean_source = re.sub(image_pattern, '', cell['source'])\n",
    "            current_markdown.append(clean_source)\n",
    "        elif cell['cell_type'] == 'code':\n",
    "            paired_cells.append((current_markdown, cell))\n",
    "            current_markdown = []  # Reset markdown for the next pair\n",
    "\n",
    "    return paired_cells\n",
    "\n",
    "# === Function to Filter Relevant Cells ===\n",
    "def filter_relevant_cells(paired_cells):\n",
    "    \"\"\"\n",
    "    Combine irrelevant code and markdown cells as instructions for the next relevant code cell \n",
    "    that includes any variation of the phrase 'start code here', case insensitive.\n",
    "    \"\"\"\n",
    "    relevant_cells = []\n",
    "    accumulated_instructions = []\n",
    "\n",
    "    # Compile a regex pattern to match 'start code here' in a case-insensitive manner\n",
    "    start_code_pattern = re.compile(r'start\\s*code\\s*here', re.IGNORECASE)\n",
    "\n",
    "    for markdown_instructions, code_cell in paired_cells:\n",
    "        combined_markdown = \"\\n\".join(markdown_instructions)\n",
    "        \n",
    "        # Accumulate markdown instructions and code from irrelevant cells\n",
    "        if not start_code_pattern.search(code_cell['source']):\n",
    "            # Tag accumulated content to distinguish markdown and code\n",
    "            accumulated_instructions.append(f\"Markdown Instructions:\\n{combined_markdown}\")\n",
    "            accumulated_instructions.append(f\"Previous Code:\\n{code_cell['source']}\")\n",
    "        else:\n",
    "            # If the relevant marker is found, include accumulated instructions\n",
    "            full_instructions = \"\\n\\n\".join(accumulated_instructions) + f\"\\n\\nMarkdown Instructions:\\n{combined_markdown}\"\n",
    "            relevant_cells.append(([full_instructions], code_cell))\n",
    "            accumulated_instructions = []  # Reset the accumulated instructions\n",
    "\n",
    "    return relevant_cells\n",
    "\n",
    "# === Function to Generate Prompt ===\n",
    "def generate_prompt(markdown_instructions, code):\n",
    "    \"\"\"\n",
    "    Generate a prompt for the OpenAI API based on accumulated markdown instructions and \n",
    "    previous code cell content, clearly distinguishing their roles.\n",
    "    \"\"\"\n",
    "    instructions_text = \"\\n\".join(markdown_instructions)\n",
    "\n",
    "    prompt = (\n",
    "    f\"Below are the instructions and previous code:\\n\\n\"\n",
    "    f\"{instructions_text}\\n\\n\"\n",
    "    f\"Current Code Cell:\\n{code}\\n\\n\"\n",
    "    f\"Complete the code in the 'Current Code Cell' based on the instructions. \"\n",
    "    f\"Return only the code itself, without any markdown, explanations, or code fences.\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def complete_code(client, assistant_id, model, prompt, i):\n",
    "    \"\"\"\n",
    "    Completes code based on the provided prompt using OpenAI's Assistant API.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Create a new thread for this interaction\n",
    "        thread = client.beta.threads.create()\n",
    "        thread_id = thread.id\n",
    "        logging.info(f\"Created new thread with ID: {thread_id} for cell {i}\")\n",
    "\n",
    "        # Send the User Message to the Assistant\n",
    "        client.beta.threads.messages.create(\n",
    "            thread_id=thread_id,\n",
    "            role=\"user\",\n",
    "            content=prompt\n",
    "        )\n",
    "        logging.info(\"Message sent to the assistant.\")\n",
    "\n",
    "        # Run the Assistant\n",
    "        run = client.beta.threads.runs.create(\n",
    "            thread_id=thread_id,\n",
    "            model=model,\n",
    "            assistant_id=assistant_id,\n",
    "            # Updated instructions to request code within code fences\n",
    "            instructions=\"As an expert code assistant, complete the provided code cell based on the instructions and previous code. Return the completed code enclosed within code fences (```python ... ```) to indicate the code.\"\n",
    "        )\n",
    "        logging.info(f\"Assistant run initiated with Run ID: {run.id}\")\n",
    "\n",
    "        # Wait for the Assistant to Complete the Run\n",
    "        while True:\n",
    "            try:\n",
    "                run_status = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run.id)\n",
    "                if run_status.status == 'failed':\n",
    "                    logging.error(f\"Run failed with error: {run_status.error}\")\n",
    "                    return \"\"\n",
    "                if run_status.completed_at:\n",
    "                    elapsed_time = run_status.completed_at - run_status.created_at\n",
    "                    formatted_elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "                    print(f\"Run completed in {formatted_elapsed_time}\")\n",
    "                    logging.info(f\"Run completed in {formatted_elapsed_time}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An error occurred while retrieving the run: {e}\")\n",
    "                return \"\"\n",
    "            logging.info(\"Waiting for run to complete...\")\n",
    "            time.sleep(5)  # Adjust sleep interval as needed\n",
    "\n",
    "        # Retrieve the Assistant's Response\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
    "        if messages.data:\n",
    "            # Find the last assistant message\n",
    "            for message in reversed(messages.data):\n",
    "                if message.role == 'assistant':\n",
    "                    last_message = message\n",
    "                    break\n",
    "            else:\n",
    "                logging.error(\"No assistant messages found in the thread after the run.\")\n",
    "                return \"\"\n",
    "\n",
    "            # Log the entire content for debugging\n",
    "            logging.info(f\"Assistant Response Content: {last_message.content}\")\n",
    "\n",
    "            # Extract and return whatever content is provided as the code\n",
    "            completed_code = ''\n",
    "            content_blocks = last_message.content  # This is a list of content blocks\n",
    "            for block in content_blocks:\n",
    "                logging.info(f\"Processing content block: {block}\")\n",
    "                if block.type == 'text':\n",
    "                    # Directly use the text content as the completed code\n",
    "                    completed_code += block.text.value + '\\n'\n",
    "                elif block.type == 'code':\n",
    "                    # If the assistant returns a code block, handle it similarly\n",
    "                    completed_code += block.code.code.value + '\\n'\n",
    "\n",
    "            completed_code = completed_code.strip()\n",
    "            return completed_code\n",
    "        else:\n",
    "            logging.error(\"No messages found in the thread after the run.\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred in complete_code: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def update_notebook(notebook, paired_cells):\n",
    "    \"\"\"Update the notebook with completed code in code cells.\"\"\"\n",
    "    i = 0\n",
    "    for markdown_instructions, code_cell in paired_cells:\n",
    "        current_code = code_cell['source']\n",
    "        prompt = generate_prompt(markdown_instructions, current_code)\n",
    "        # Pass the index `i` to `complete_code`\n",
    "        completed_code = complete_code(client, assistant_id, model, prompt, i)\n",
    "        if completed_code:\n",
    "            completed_code = re.sub(r'```(?:python)?\\n(.*?)```', r'\\1', completed_code, flags=re.DOTALL).strip()\n",
    "            print(f\"Completed code for cell {i}\")\n",
    "            logging.info(f\"Completed code for cell {i}\")\n",
    "            code_cell['source'] = completed_code\n",
    "        else:\n",
    "            logging.warning(f\"No completion received for cell {i}\")\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "# === Function to Save the Updated Notebook ===\n",
    "def save_notebook(notebook, original_path):\n",
    "    \"\"\"Save the updated notebook with a new name to avoid overwriting the original.\"\"\"\n",
    "    directory, original_filename = os.path.split(original_path)\n",
    "    new_filename = f\"updated_{original_filename}\"\n",
    "    new_path = os.path.join(directory, new_filename)\n",
    "    with open(new_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(notebook, f)\n",
    "    print(f\"Notebook saved as {new_path}\")\n",
    "    logging.info(f\"Notebook saved as {new_path}\")\n",
    "\n",
    "# === Main Processing Function ===\n",
    "def process_notebook(file_path):\n",
    "    \"\"\"Complete the entire process: read, filter, update, and save the notebook.\"\"\"\n",
    "    notebook = read_notebook(file_path)\n",
    "    input(\"read notebook finished, Press Enter to continue...\")\n",
    "    paired_cells = extract_cells(notebook)\n",
    "    input(\"cells extracted finished, Press Enter to continue...\")\n",
    "    relevant_cells = filter_relevant_cells(paired_cells)  \n",
    "    input(\"cells filtered for relevance. Press Enter to continue...\")\n",
    "    update_notebook(notebook, relevant_cells)\n",
    "    input(\"notebook updated with completions. Press Enter to continue...\")\n",
    "    save_notebook(notebook, file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:33:28,938 - INFO - HTTP Request: POST https://api.openai.com/v1/threads \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:28,940 - INFO - Created new thread with ID: thread_n7HWLILb8EbaKnKTnTqb9qBC for cell 0\n",
      "2024-10-16 14:33:29,190 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_n7HWLILb8EbaKnKTnTqb9qBC/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:29,191 - INFO - Message sent to the assistant.\n",
      "2024-10-16 14:33:29,906 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_n7HWLILb8EbaKnKTnTqb9qBC/runs \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:29,907 - INFO - Assistant run initiated with Run ID: run_1vvGk3SlYp38LwE9ojKXOnlj\n",
      "2024-10-16 14:33:30,128 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_n7HWLILb8EbaKnKTnTqb9qBC/runs/run_1vvGk3SlYp38LwE9ojKXOnlj \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:30,129 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:33:35,355 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_n7HWLILb8EbaKnKTnTqb9qBC/runs/run_1vvGk3SlYp38LwE9ojKXOnlj \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:35,357 - INFO - Run completed in 00:00:05\n",
      "2024-10-16 14:33:35,562 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_n7HWLILb8EbaKnKTnTqb9qBC/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed in 00:00:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:33:35,564 - INFO - Assistant Response Content: [TextContentBlock(text=Text(annotations=[], value='class InputEmbeddings(nn.Module):\\n\\n    def __init__(self, d_model: int, vocab_size: int) -> None:\\n        super().__init__()\\n        self.d_model = d_model\\n        self.vocab_size = vocab_size\\n        ### START CODE HERE ###\\n        self.embedding = nn.Embedding(vocab_size, d_model)  # Use the nn.Embedding function here, use vocab_size and d_model as arguments\\n        ### END CODE HERE ###\\n\\n    def forward(self, x):\\n        # (batch, seq_len) --> (batch, seq_len, d_model)\\n        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\\n\\n        ### START CODE HERE ###\\n        x = self.embedding(x)  # apply the self.embedding layer\\n        x = x * math.sqrt(self.d_model)  # multiply output of embedding layer with square root of d_model\\n        ### END CODE HERE ###\\n\\n        return x'), type='text')]\n",
      "2024-10-16 14:33:35,565 - INFO - Processing content block: TextContentBlock(text=Text(annotations=[], value='class InputEmbeddings(nn.Module):\\n\\n    def __init__(self, d_model: int, vocab_size: int) -> None:\\n        super().__init__()\\n        self.d_model = d_model\\n        self.vocab_size = vocab_size\\n        ### START CODE HERE ###\\n        self.embedding = nn.Embedding(vocab_size, d_model)  # Use the nn.Embedding function here, use vocab_size and d_model as arguments\\n        ### END CODE HERE ###\\n\\n    def forward(self, x):\\n        # (batch, seq_len) --> (batch, seq_len, d_model)\\n        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\\n\\n        ### START CODE HERE ###\\n        x = self.embedding(x)  # apply the self.embedding layer\\n        x = x * math.sqrt(self.d_model)  # multiply output of embedding layer with square root of d_model\\n        ### END CODE HERE ###\\n\\n        return x'), type='text')\n",
      "2024-10-16 14:33:35,566 - INFO - Completed code for cell 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed code for cell 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:33:35,780 - INFO - HTTP Request: POST https://api.openai.com/v1/threads \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:35,781 - INFO - Created new thread with ID: thread_LYIBNeNdGqtZzIg31HDBY5DV for cell 1\n",
      "2024-10-16 14:33:40,511 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_LYIBNeNdGqtZzIg31HDBY5DV/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:40,513 - INFO - Message sent to the assistant.\n",
      "2024-10-16 14:33:41,130 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_LYIBNeNdGqtZzIg31HDBY5DV/runs \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:41,131 - INFO - Assistant run initiated with Run ID: run_Knoxho55XG0yozJKg9nyVkwf\n",
      "2024-10-16 14:33:41,405 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_LYIBNeNdGqtZzIg31HDBY5DV/runs/run_Knoxho55XG0yozJKg9nyVkwf \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:41,407 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:33:46,764 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_LYIBNeNdGqtZzIg31HDBY5DV/runs/run_Knoxho55XG0yozJKg9nyVkwf \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:46,765 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:33:52,035 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_LYIBNeNdGqtZzIg31HDBY5DV/runs/run_Knoxho55XG0yozJKg9nyVkwf \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:52,036 - INFO - Run completed in 00:00:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed in 00:00:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:33:52,424 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_LYIBNeNdGqtZzIg31HDBY5DV/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:52,426 - INFO - Assistant Response Content: [TextContentBlock(text=Text(annotations=[], value='class LayerNormalization(nn.Module):\\n\\n    def __init__(self, features: int, eps:float=10**-6) -> None:\\n        super().__init__()\\n        self.eps = eps\\n        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\\n        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\\n\\n    def forward(self, x):\\n        # x: (batch, seq_len, hidden_size)\\n         # Keep the dimension for broadcasting\\n        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\\n        # Keep the dimension for broadcasting\\n        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\\n\\n        ### START CODE HERE ###\\n        x = (x - mean) / (std + self.eps)\\n\\n        x = x * self.alpha + self.bias\\n        ### END CODE HERE ###\\n\\n        return x'), type='text')]\n",
      "2024-10-16 14:33:52,426 - INFO - Processing content block: TextContentBlock(text=Text(annotations=[], value='class LayerNormalization(nn.Module):\\n\\n    def __init__(self, features: int, eps:float=10**-6) -> None:\\n        super().__init__()\\n        self.eps = eps\\n        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\\n        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\\n\\n    def forward(self, x):\\n        # x: (batch, seq_len, hidden_size)\\n         # Keep the dimension for broadcasting\\n        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\\n        # Keep the dimension for broadcasting\\n        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\\n\\n        ### START CODE HERE ###\\n        x = (x - mean) / (std + self.eps)\\n\\n        x = x * self.alpha + self.bias\\n        ### END CODE HERE ###\\n\\n        return x'), type='text')\n",
      "2024-10-16 14:33:52,427 - INFO - Completed code for cell 1\n",
      "2024-10-16 14:33:52,631 - INFO - HTTP Request: POST https://api.openai.com/v1/threads \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed code for cell 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:33:52,633 - INFO - Created new thread with ID: thread_6c0ckNM0VVfJoppe2ZCNegM1 for cell 2\n",
      "2024-10-16 14:33:52,885 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_6c0ckNM0VVfJoppe2ZCNegM1/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:52,887 - INFO - Message sent to the assistant.\n",
      "2024-10-16 14:33:53,410 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_6c0ckNM0VVfJoppe2ZCNegM1/runs \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:53,411 - INFO - Assistant run initiated with Run ID: run_7BX5druGhQr6fXerYUml8d9s\n",
      "2024-10-16 14:33:53,637 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_6c0ckNM0VVfJoppe2ZCNegM1/runs/run_7BX5druGhQr6fXerYUml8d9s \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:53,639 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:33:58,883 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_6c0ckNM0VVfJoppe2ZCNegM1/runs/run_7BX5druGhQr6fXerYUml8d9s \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:58,884 - INFO - Run completed in 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed in 00:00:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:33:59,157 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_6c0ckNM0VVfJoppe2ZCNegM1/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:59,159 - INFO - Assistant Response Content: [TextContentBlock(text=Text(annotations=[], value='class ResidualConnection(nn.Module):\\n\\n    def __init__(self, features: int, dropout: float) -> None:\\n        super().__init__()\\n        self.dropout = nn.Dropout(dropout)\\n        self.norm = LayerNormalization(features)\\n\\n    def forward(self, x, sublayer):\\n        ### START CODE HERE ###\\n        out = self.norm(x)  # Apply the normalization layer on x\\n        out = sublayer(out)  # Apply the sublayer passed in the arguments on out\\n        out = self.dropout(out)  # Apply the dropout on the output of the sublayer\\n        out = out + x  # Add the input x to the output of the dropout layer\\n        ### END CODE HERE ###\\n        return out'), type='text')]\n",
      "2024-10-16 14:33:59,159 - INFO - Processing content block: TextContentBlock(text=Text(annotations=[], value='class ResidualConnection(nn.Module):\\n\\n    def __init__(self, features: int, dropout: float) -> None:\\n        super().__init__()\\n        self.dropout = nn.Dropout(dropout)\\n        self.norm = LayerNormalization(features)\\n\\n    def forward(self, x, sublayer):\\n        ### START CODE HERE ###\\n        out = self.norm(x)  # Apply the normalization layer on x\\n        out = sublayer(out)  # Apply the sublayer passed in the arguments on out\\n        out = self.dropout(out)  # Apply the dropout on the output of the sublayer\\n        out = out + x  # Add the input x to the output of the dropout layer\\n        ### END CODE HERE ###\\n        return out'), type='text')\n",
      "2024-10-16 14:33:59,160 - INFO - Completed code for cell 2\n",
      "2024-10-16 14:33:59,363 - INFO - HTTP Request: POST https://api.openai.com/v1/threads \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed code for cell 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:33:59,365 - INFO - Created new thread with ID: thread_pKdEy8tzgrfWnDNKHQQVEqa8 for cell 3\n",
      "2024-10-16 14:33:59,605 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_pKdEy8tzgrfWnDNKHQQVEqa8/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:33:59,607 - INFO - Message sent to the assistant.\n",
      "2024-10-16 14:34:00,858 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_pKdEy8tzgrfWnDNKHQQVEqa8/runs \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:00,859 - INFO - Assistant run initiated with Run ID: run_baT07V2XXdHTK54ldeNQmg5e\n",
      "2024-10-16 14:34:01,322 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_pKdEy8tzgrfWnDNKHQQVEqa8/runs/run_baT07V2XXdHTK54ldeNQmg5e \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:01,324 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:34:06,579 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_pKdEy8tzgrfWnDNKHQQVEqa8/runs/run_baT07V2XXdHTK54ldeNQmg5e \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:06,580 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:34:12,154 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_pKdEy8tzgrfWnDNKHQQVEqa8/runs/run_baT07V2XXdHTK54ldeNQmg5e \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:12,155 - INFO - Run completed in 00:00:09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed in 00:00:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:34:12,923 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_pKdEy8tzgrfWnDNKHQQVEqa8/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:12,925 - INFO - Assistant Response Content: [TextContentBlock(text=Text(annotations=[], value='class FeedForwardBlock(nn.Module):\\n\\n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\\n        super().__init__()\\n        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\\n        self.dropout = nn.Dropout(dropout)\\n        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\\n\\n    def forward(self, x):\\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\\n\\n        ### START CODE HERE ###\\n        x = self.linear_1(x)           # Apply linear1 layer\\n        x = F.relu(x)                  # Apply relu activation\\n        x = self.dropout(x)            # Apply dropout\\n        x = self.linear_2(x)           # Apply linear2 layer\\n        ### END CODE HERE ###\\n\\n        return x'), type='text')]\n",
      "2024-10-16 14:34:12,925 - INFO - Processing content block: TextContentBlock(text=Text(annotations=[], value='class FeedForwardBlock(nn.Module):\\n\\n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\\n        super().__init__()\\n        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\\n        self.dropout = nn.Dropout(dropout)\\n        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\\n\\n    def forward(self, x):\\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\\n\\n        ### START CODE HERE ###\\n        x = self.linear_1(x)           # Apply linear1 layer\\n        x = F.relu(x)                  # Apply relu activation\\n        x = self.dropout(x)            # Apply dropout\\n        x = self.linear_2(x)           # Apply linear2 layer\\n        ### END CODE HERE ###\\n\\n        return x'), type='text')\n",
      "2024-10-16 14:34:12,926 - INFO - Completed code for cell 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed code for cell 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:34:13,155 - INFO - HTTP Request: POST https://api.openai.com/v1/threads \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:13,156 - INFO - Created new thread with ID: thread_ELcGjPbGmEjZExmyLuXF858g for cell 4\n",
      "2024-10-16 14:34:13,437 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_ELcGjPbGmEjZExmyLuXF858g/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:13,566 - INFO - Message sent to the assistant.\n",
      "2024-10-16 14:34:14,077 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_ELcGjPbGmEjZExmyLuXF858g/runs \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:14,078 - INFO - Assistant run initiated with Run ID: run_vHa6MNHEBKImQQwFTHYc4XtZ\n",
      "2024-10-16 14:34:14,313 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_ELcGjPbGmEjZExmyLuXF858g/runs/run_vHa6MNHEBKImQQwFTHYc4XtZ \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:14,315 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:34:19,594 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_ELcGjPbGmEjZExmyLuXF858g/runs/run_vHa6MNHEBKImQQwFTHYc4XtZ \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:19,595 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:34:24,826 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_ELcGjPbGmEjZExmyLuXF858g/runs/run_vHa6MNHEBKImQQwFTHYc4XtZ \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:24,828 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:34:30,105 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_ELcGjPbGmEjZExmyLuXF858g/runs/run_vHa6MNHEBKImQQwFTHYc4XtZ \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:30,107 - INFO - Run completed in 00:00:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed in 00:00:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:34:30,339 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_ELcGjPbGmEjZExmyLuXF858g/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:30,341 - INFO - Assistant Response Content: [TextContentBlock(text=Text(annotations=[], value='class MultiHeadAttentionBlock(nn.Module):\\n\\n    def __init__(self, d_model: int, h: int, dropout: float) -> None:\\n        super().__init__()\\n\\n        self.d_model = d_model # Embedding vector size\\n        self.h = h # Number of heads that we are going to use for multihead attention\\n\\n        assert d_model % h == 0, \"d_model is not divisible by h\"\\n\\n        self.d_k = d_model // h # Dimension of vector seen by each head\\n\\n        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\\n\\n        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\\n        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\\n\\n        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\\n\\n        self.dropout = nn.Dropout(dropout)\\n\\n    @staticmethod\\n    def attention(query, key, value, mask, dropout: nn.Dropout):\\n        d_k = query.shape[-1]\\n\\n        attention_scores = query @ key.transpose(-2, -1) # Attention scores\\n        attention_scores = attention_scores / math.sqrt(d_k) # Scale the scores\\n\\n        if mask is not None:\\n            attention_scores.masked_fill_(mask == 0, -1e9)\\n\\n        attention_scores = F.softmax(attention_scores, dim=-1) # Apply softmax\\n\\n        if dropout is not None:\\n            attention_scores = dropout(attention_scores)\\n\\n        out = attention_scores @ value # Weighted sum of values\\n\\n        return out, attention_scores\\n\\n    def forward(self, q, k, v, mask):\\n\\n        query = self.w_q(q) # Apply the w_q layer on q\\n        key = self.w_k(k)   # Apply the w_k layer on k\\n        value = self.w_v(v) # Apply the w_v layer on v\\n\\n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\\n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2) # change shape of key\\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2) # change shape of value\\n\\n        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\\n\\n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\\n\\n        x = self.w_o(x) # Apply the w_o layer on x\\n\\n        return x'), type='text')]\n",
      "2024-10-16 14:34:30,342 - INFO - Processing content block: TextContentBlock(text=Text(annotations=[], value='class MultiHeadAttentionBlock(nn.Module):\\n\\n    def __init__(self, d_model: int, h: int, dropout: float) -> None:\\n        super().__init__()\\n\\n        self.d_model = d_model # Embedding vector size\\n        self.h = h # Number of heads that we are going to use for multihead attention\\n\\n        assert d_model % h == 0, \"d_model is not divisible by h\"\\n\\n        self.d_k = d_model // h # Dimension of vector seen by each head\\n\\n        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\\n\\n        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\\n        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\\n\\n        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\\n\\n        self.dropout = nn.Dropout(dropout)\\n\\n    @staticmethod\\n    def attention(query, key, value, mask, dropout: nn.Dropout):\\n        d_k = query.shape[-1]\\n\\n        attention_scores = query @ key.transpose(-2, -1) # Attention scores\\n        attention_scores = attention_scores / math.sqrt(d_k) # Scale the scores\\n\\n        if mask is not None:\\n            attention_scores.masked_fill_(mask == 0, -1e9)\\n\\n        attention_scores = F.softmax(attention_scores, dim=-1) # Apply softmax\\n\\n        if dropout is not None:\\n            attention_scores = dropout(attention_scores)\\n\\n        out = attention_scores @ value # Weighted sum of values\\n\\n        return out, attention_scores\\n\\n    def forward(self, q, k, v, mask):\\n\\n        query = self.w_q(q) # Apply the w_q layer on q\\n        key = self.w_k(k)   # Apply the w_k layer on k\\n        value = self.w_v(v) # Apply the w_v layer on v\\n\\n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\\n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2) # change shape of key\\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2) # change shape of value\\n\\n        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\\n\\n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\\n\\n        x = self.w_o(x) # Apply the w_o layer on x\\n\\n        return x'), type='text')\n",
      "2024-10-16 14:34:30,342 - INFO - Completed code for cell 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed code for cell 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:34:30,578 - INFO - HTTP Request: POST https://api.openai.com/v1/threads \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:30,580 - INFO - Created new thread with ID: thread_w5I6MneQTkubMSbJRX4XyPXt for cell 5\n",
      "2024-10-16 14:34:30,879 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_w5I6MneQTkubMSbJRX4XyPXt/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:31,005 - INFO - Message sent to the assistant.\n",
      "2024-10-16 14:34:32,230 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_w5I6MneQTkubMSbJRX4XyPXt/runs \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:32,232 - INFO - Assistant run initiated with Run ID: run_AgEfJNPWRz5FKY4o1bW0g1iC\n",
      "2024-10-16 14:34:32,479 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_w5I6MneQTkubMSbJRX4XyPXt/runs/run_AgEfJNPWRz5FKY4o1bW0g1iC \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:32,481 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:34:37,808 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_w5I6MneQTkubMSbJRX4XyPXt/runs/run_AgEfJNPWRz5FKY4o1bW0g1iC \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:37,809 - INFO - Run completed in 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed in 00:00:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:34:38,381 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_w5I6MneQTkubMSbJRX4XyPXt/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:38,506 - INFO - Assistant Response Content: [TextContentBlock(text=Text(annotations=[], value='class ProjectionLayer(nn.Module):\\n\\n    def __init__(self, d_model, vocab_size) -> None:\\n        super().__init__()\\n        self.proj = nn.Linear(d_model, vocab_size)\\n\\n    def forward(self, x) -> None:\\n        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\\n\\n        ### START CODE HERE ###\\n        x = self.proj(x)  # Apply the linear layer for projection here (proj)\\n        ### END CODE HERE ###\\n\\n        return x'), type='text')]\n",
      "2024-10-16 14:34:38,506 - INFO - Processing content block: TextContentBlock(text=Text(annotations=[], value='class ProjectionLayer(nn.Module):\\n\\n    def __init__(self, d_model, vocab_size) -> None:\\n        super().__init__()\\n        self.proj = nn.Linear(d_model, vocab_size)\\n\\n    def forward(self, x) -> None:\\n        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\\n\\n        ### START CODE HERE ###\\n        x = self.proj(x)  # Apply the linear layer for projection here (proj)\\n        ### END CODE HERE ###\\n\\n        return x'), type='text')\n",
      "2024-10-16 14:34:38,507 - INFO - Completed code for cell 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed code for cell 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:34:38,828 - INFO - HTTP Request: POST https://api.openai.com/v1/threads \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:38,829 - INFO - Created new thread with ID: thread_yAdBF2C0oQ03flj2SI5agE1T for cell 6\n",
      "2024-10-16 14:34:39,108 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_yAdBF2C0oQ03flj2SI5agE1T/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:39,109 - INFO - Message sent to the assistant.\n",
      "2024-10-16 14:34:39,715 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_yAdBF2C0oQ03flj2SI5agE1T/runs \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:39,716 - INFO - Assistant run initiated with Run ID: run_ygY1l9QriY13rEgtZf1A4XDc\n",
      "2024-10-16 14:34:40,009 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_yAdBF2C0oQ03flj2SI5agE1T/runs/run_ygY1l9QriY13rEgtZf1A4XDc \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:40,010 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:34:45,344 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_yAdBF2C0oQ03flj2SI5agE1T/runs/run_ygY1l9QriY13rEgtZf1A4XDc \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:45,346 - INFO - Run completed in 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed in 00:00:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:34:45,614 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_yAdBF2C0oQ03flj2SI5agE1T/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:45,616 - INFO - Assistant Response Content: [TextContentBlock(text=Text(annotations=[], value='class Transformer(nn.Module):\\n\\n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\\n        super().__init__()\\n\\n        self.encoder = encoder\\n        self.decoder = decoder\\n\\n        self.src_embed = src_embed\\n        self.tgt_embed = tgt_embed\\n\\n        self.src_pos = src_pos\\n        self.tgt_pos = tgt_pos\\n\\n        self.projection_layer = projection_layer\\n\\n    def encode(self, src, src_mask):\\n\\n        ### START CODE HERE ###\\n        src = self.src_embed(src)            # Apply the source embedding layer on the encoder input\\n        src = self.src_pos(src)               # Apply the positional encoding on the output\\n        src = self.encoder(src, src_mask)     # Apply the encoder on the output of the positional encoding layer along with the src_mask (src, src_mask)\\n        ### END CODE HERE ###\\n\\n        return src\\n\\n    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\\n\\n        ### START CODE HERE ###\\n        tgt = self.tgt_embed(tgt)             # Apply the target embedding layer on the decoder input\\n        tgt = self.tgt_pos(tgt)                # Apply the positional encoding on the output of the target embedding layer\\n\\n        tgt = self.decoder(tgt, encoder_output, src_mask, tgt_mask)  # Apply the decoder on the output of the positional encoding layer\\n        ### END CODE HERE ###\\n\\n        return tgt\\n\\n    def project(self, x):\\n        # (batch, seq_len, vocab_size)\\n        ### START CODE HERE ###\\n        x = self.projection_layer(x)           # Apply the projection layer on the input x\\n        ### END CODE HERE ###\\n\\n        return x'), type='text')]\n",
      "2024-10-16 14:34:45,617 - INFO - Processing content block: TextContentBlock(text=Text(annotations=[], value='class Transformer(nn.Module):\\n\\n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\\n        super().__init__()\\n\\n        self.encoder = encoder\\n        self.decoder = decoder\\n\\n        self.src_embed = src_embed\\n        self.tgt_embed = tgt_embed\\n\\n        self.src_pos = src_pos\\n        self.tgt_pos = tgt_pos\\n\\n        self.projection_layer = projection_layer\\n\\n    def encode(self, src, src_mask):\\n\\n        ### START CODE HERE ###\\n        src = self.src_embed(src)            # Apply the source embedding layer on the encoder input\\n        src = self.src_pos(src)               # Apply the positional encoding on the output\\n        src = self.encoder(src, src_mask)     # Apply the encoder on the output of the positional encoding layer along with the src_mask (src, src_mask)\\n        ### END CODE HERE ###\\n\\n        return src\\n\\n    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\\n\\n        ### START CODE HERE ###\\n        tgt = self.tgt_embed(tgt)             # Apply the target embedding layer on the decoder input\\n        tgt = self.tgt_pos(tgt)                # Apply the positional encoding on the output of the target embedding layer\\n\\n        tgt = self.decoder(tgt, encoder_output, src_mask, tgt_mask)  # Apply the decoder on the output of the positional encoding layer\\n        ### END CODE HERE ###\\n\\n        return tgt\\n\\n    def project(self, x):\\n        # (batch, seq_len, vocab_size)\\n        ### START CODE HERE ###\\n        x = self.projection_layer(x)           # Apply the projection layer on the input x\\n        ### END CODE HERE ###\\n\\n        return x'), type='text')\n",
      "2024-10-16 14:34:45,617 - INFO - Completed code for cell 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed code for cell 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:34:45,902 - INFO - HTTP Request: POST https://api.openai.com/v1/threads \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:45,903 - INFO - Created new thread with ID: thread_VTQ875jTkDAdqw3Cm9lgpFtI for cell 7\n",
      "2024-10-16 14:34:46,463 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_VTQ875jTkDAdqw3Cm9lgpFtI/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:46,465 - INFO - Message sent to the assistant.\n",
      "2024-10-16 14:34:47,196 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_VTQ875jTkDAdqw3Cm9lgpFtI/runs \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:47,197 - INFO - Assistant run initiated with Run ID: run_36mUT8X0VehVMWEbiESHE2qK\n",
      "2024-10-16 14:34:47,592 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_VTQ875jTkDAdqw3Cm9lgpFtI/runs/run_36mUT8X0VehVMWEbiESHE2qK \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:47,593 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:34:52,959 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_VTQ875jTkDAdqw3Cm9lgpFtI/runs/run_36mUT8X0VehVMWEbiESHE2qK \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:52,961 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:34:58,194 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_VTQ875jTkDAdqw3Cm9lgpFtI/runs/run_36mUT8X0VehVMWEbiESHE2qK \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:34:58,195 - INFO - Waiting for run to complete...\n",
      "2024-10-16 14:35:03,424 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_VTQ875jTkDAdqw3Cm9lgpFtI/runs/run_36mUT8X0VehVMWEbiESHE2qK \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:35:03,426 - INFO - Run completed in 00:00:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed in 00:00:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 14:35:03,647 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_VTQ875jTkDAdqw3Cm9lgpFtI/messages \"HTTP/1.1 200 OK\"\n",
      "2024-10-16 14:35:03,649 - INFO - Assistant Response Content: [TextContentBlock(text=Text(annotations=[], value='def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\\n\\n    ### START CODE HERE ###\\n    # Create the embedding layers\\n    src_embed = EmbeddingLayer(d_model, src_vocab_size)  # Create an embedding layer for the source language.\\n    tgt_embed = EmbeddingLayer(d_model, tgt_vocab_size)  # Create an embedding layer for the target language.\\n    ### END CODE HERE ###\\n\\n    # Create the positional encoding layers\\n    ### START CODE HERE ###\\n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)  # Positional encoding for source\\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)  # Positional encoding for target\\n    ### END CODE HERE ###\\n\\n    # Her we create a list of the encoder blocks that we will use to create the complete encoder of our transformer\\n    encoder_blocks = []\\n\\n    for _ in range(N):\\n        ### START CODE HERE ###\\n        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)  # Create the encoder self attention block\\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)  # Create the feed_forward_block\\n        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block)  # Create an encoder block\\n        encoder_blocks.append(encoder_block)  # Append to the list\\n        ### END CODE HERE ###\\n\\n    # Her we create the decoder blocks\\n    decoder_blocks = []\\n    for _ in range(N):\\n        ### START CODE HERE ###\\n        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)  # Create the decoder self attention block\\n        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)  # Create the decoder cross attention block\\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)  # Create the feed_forward_block\\n        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block)  # Create a decoder block\\n        decoder_blocks.append(decoder_block)  # Append to the list\\n        ### END CODE HERE ###\\n\\n    # Here you create the encoder and decoder\\n    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))  # Create the encoder\\n    ### START CODE HERE ###\\n    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))  # Create the decoder\\n    ### END CODE HERE ###\\n\\n    # Create the projection layer \\n    ### START CODE HERE ###\\n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)  # Use target vocab size for projection\\n    ### END CODE HERE ###\\n\\n    # Finally create your transformer\\n    ### START CODE HERE ###\\n    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)  # Instantiate transformer\\n    ### END CODE HERE ###\\n\\n    # Initialize the parameters\\n    for p in transformer.parameters():\\n        if p.dim() > 1:\\n            nn.init.xavier_uniform_(p)\\n\\n    return transformer'), type='text')]\n",
      "2024-10-16 14:35:03,649 - INFO - Processing content block: TextContentBlock(text=Text(annotations=[], value='def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\\n\\n    ### START CODE HERE ###\\n    # Create the embedding layers\\n    src_embed = EmbeddingLayer(d_model, src_vocab_size)  # Create an embedding layer for the source language.\\n    tgt_embed = EmbeddingLayer(d_model, tgt_vocab_size)  # Create an embedding layer for the target language.\\n    ### END CODE HERE ###\\n\\n    # Create the positional encoding layers\\n    ### START CODE HERE ###\\n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)  # Positional encoding for source\\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)  # Positional encoding for target\\n    ### END CODE HERE ###\\n\\n    # Her we create a list of the encoder blocks that we will use to create the complete encoder of our transformer\\n    encoder_blocks = []\\n\\n    for _ in range(N):\\n        ### START CODE HERE ###\\n        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)  # Create the encoder self attention block\\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)  # Create the feed_forward_block\\n        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block)  # Create an encoder block\\n        encoder_blocks.append(encoder_block)  # Append to the list\\n        ### END CODE HERE ###\\n\\n    # Her we create the decoder blocks\\n    decoder_blocks = []\\n    for _ in range(N):\\n        ### START CODE HERE ###\\n        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)  # Create the decoder self attention block\\n        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)  # Create the decoder cross attention block\\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)  # Create the feed_forward_block\\n        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block)  # Create a decoder block\\n        decoder_blocks.append(decoder_block)  # Append to the list\\n        ### END CODE HERE ###\\n\\n    # Here you create the encoder and decoder\\n    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))  # Create the encoder\\n    ### START CODE HERE ###\\n    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))  # Create the decoder\\n    ### END CODE HERE ###\\n\\n    # Create the projection layer \\n    ### START CODE HERE ###\\n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)  # Use target vocab size for projection\\n    ### END CODE HERE ###\\n\\n    # Finally create your transformer\\n    ### START CODE HERE ###\\n    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)  # Instantiate transformer\\n    ### END CODE HERE ###\\n\\n    # Initialize the parameters\\n    for p in transformer.parameters():\\n        if p.dim() > 1:\\n            nn.init.xavier_uniform_(p)\\n\\n    return transformer'), type='text')\n",
      "2024-10-16 14:35:03,650 - INFO - Completed code for cell 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed code for cell 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 15:02:45,999 - INFO - Notebook saved as updated_jupyter.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook saved as updated_jupyter.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "file_path = \"jupyter.ipynb\"\n",
    "process_notebook(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
